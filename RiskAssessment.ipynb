{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade langchain\n",
    "# pip install pypdf\n",
    "# pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_llm = AzureChatOpenAI(\n",
    "    temperature = 0.2,\n",
    "    azure_deployment = 'deployment-text-risk-assessment',\n",
    "    # azure_endpoint = os.getenv(),\n",
    "    # openai_api_type = os.getenv()\n",
    "    openai_api_version = '2023-08-01-preview', \n",
    "    openai_api_key = os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    openai_api_base = 'https://instance-text-risk-assessment.openai.azure.com/',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tool #1 - BingSearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import BingSearchAPIWrapper\n",
    "from langchain.tools.bing_search.tool import BingSearchRun\n",
    "from langchain.agents import AgentType, initialize_agent, Tool\n",
    "\n",
    "# os.environ['BING_SUBSCRIPTION_KEY']=os.getenv(\"BING_SUBSCRIPTION_KEY\")\n",
    "# os.environ['BING_SUBSCRIPTION_KEY']= \"28f56faa9d3245648b585190be684fdb\"\n",
    "os.environ['BING_SUBSCRIPTION_KEY']= os.getenv('BING_SUBSCRIPTION_KEY')\n",
    "os.environ[\"BING_SEARCH_URL\"] = \"https://api.bing.microsoft.com/v7.0/search\"\n",
    "\n",
    "searchWrapper = BingSearchAPIWrapper()\n",
    "tool_BingSearch = BingSearchRun(api_wrapper=searchWrapper)\n",
    "\n",
    "\n",
    "query_websearch = \"what is django\"\n",
    "query_websearch = \"what is api gateway\"\n",
    "# result = tool_BingSearch(query_websearch)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Knowlege Base - Step 1: upload & split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfilepath = ('./data/risk_assessment')\\nloader = PyPDFDirectoryLoader(filepath)\\n\\ndocuments = loader.load()\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\\ndocuments_chunks = text_splitter.split_documents(documents_original)\\n\\ndocuments_chunks\\n\\n\""
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "\"\"\"\n",
    "filepath = ('./data/risk_assessment')\n",
    "loader = PyPDFDirectoryLoader(filepath)\n",
    "\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "documents_chunks = text_splitter.split_documents(documents_original)\n",
    "\n",
    "documents_chunks\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Knowledge Base - Step 2: embedding for splitted documents, store text chunks & embeddings into vector db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITEM 3. LEGAL PROCEEDINGS\n",
      "I. SUPPLEMENTAL INFORMATION: We discuss certain legal proceedings in Note 10 to our Consolidated Financial Statements\n",
      "included in \"Item 8. Financial Statements and Supplementary Data,\" which is captioned \"Contingencies,\" under the sub-caption \"Legal\n",
      "Proceedings.\" We refer you to that discussion for important information concerning those legal proceedings, including the basis for\n",
      "such actions and, where known, the relief sought. We provide the following additional information concerning those legal proceedings,\n",
      "including the name of the lawsuit, the court in which the lawsuit is pending, and the date on which the petition commencing the lawsuit\n",
      "was filed.\n",
      "Prescription Opiate Litigation: In re National Prescription Opiate Litigation (MDL No. 2804) (the \"MDL\"). The MDL is pending in\n",
      "the U.S. District Court for the Northern District of Ohio and includes over 2,000 cases as of March 3, 2023. The liability phase of a\n",
      "single, two-county trial in one of the MDL cases against a number of parties, including the Company, regarding opioid dispensing\n",
      "claims resulted in a jury verdict on November 23, 2021, finding in favor of the plaintiffs as to the liability of all defendants, including\n",
      "the Company. The abatement phase of the single, two-county trial resulted in a judgment on August 17, 2022, that ordered all three\n",
      "defendants, including the Company, to pay an aggregate amount of approximately $651 million over fifteen years, on a joint and\n",
      "several liability basis, and granted the plaintiffs injunctive relief. The Company has filed an appeal with the Sixth Circuit Court of\n",
      "Appeals. The monetary aspect of the judgment is stayed pending appeal, and the injunctive portion of the judgment went into effect on\n",
      "February 20, 2023. The MDL has designated five additional single-county cases as bellwethers to proceed through discovery. In\n",
      "addition, there are over 300 other cases pending in state and federal courts throughout the country as of March 3, 2023. The case\n",
      "citations and currently scheduled trial dates, where applicable, are listed on Exhibit 99.1 to this Form 10-K.\n",
      "Opioid Settlement Framework: On November 15, 2022, the Company announced that it had agreed to a Settlement Framework to\n",
      "resolve substantially all opioids-related lawsuits filed against the Company by states, political subdivisions, and Native American tribes\n",
      "(other than the single, two-county trial on appeal to the Sixth Circuit Court of Appeals as described above), as described in more detail\n",
      "in Note 10 to the Consolidated Financial Statements. The Company now has settlement agreements with all 50 states, including four\n",
      "states that previously settled with the Company, as well as the District of Columbia, Puerto Rico, and three other U.S. territories, that\n",
      "are intended to resolve substantially all opioids-related lawsuits brought by state and local governments against the Company. The\n",
      "settlement will take effect if a sufficient number of political subdivisions also join.\n",
      "DOJ Opioid Civil Litigation: A civil complaint pending in the U.S. District Court for the District of Delaware has been filed by the\n",
      "U.S. Department of Justice (the \"DOJ\") against the Company, in which the DOJ alleges violations of the Controlled Substances Act\n",
      "related to nationwide distribution and dispensing of opioids. U.S. v. Walmart Inc., et al., USDC, Dist. of DE, 12/22/20. The Company\n",
      "filed a motion to dismiss the DOJ complaint on February 22, 2021. After the parties had fully briefed the Company's motion to dismiss,\n",
      "the DOJ filed an amended complaint on October 7, 2022. On November 7, 2022, the Company filed a partial motion to dismiss the\n",
      "amended complaint. The motion remains pending.\n",
      "Opioids Related Securities Class Actions and Derivative Litigation: Three derivative complaints and two securities class actions\n",
      "drawing heavily on the allegations of the DOJ complaint have been filed in Delaware naming the Company and various current and\n",
      "former directors and certain current and former officers as defendants. The plaintiffs in the derivative suits (in which the Company is a\n",
      "nominal defendant) allege, among other things, that the defendants breached their fiduciary duties in connection with oversight of\n",
      "opioids dispensing and distribution and that the defendants violated Section 14(a) of the Securities Exchange Act of 1934, as amended\n",
      "(the \"Exchange Act\"), and are liable for contribution under Section 10(b) of the Exchange Act in connection with the Company's\n",
      "disclosures about opioids. Two of the derivative suits have been filed in the U.S. District Court in Delaware and those suits have been\n",
      "stayed pending further developments in other opioids litigation matters. The other derivative suit has been filed in the Delaware Court\n",
      "of Chancery. The defendants in the derivative suit pending in the Delaware Court of Chancery moved to dismiss and/or to stay that\n",
      "case on December 21, 2021; the plaintiffs responded by filing an amended complaint on February 22, 2022. On April 20, 2022, the\n",
      "defendants moved to dismiss and/or stay proceedings on the amended complaint. The court held a hearing on that motion on September\n",
      "26, 2022; a ruling remains pending. The securities class actions, alleging violations of Sections 10(b) and 20(a) of the Exchange Act\n",
      "regarding the Company's disclosures with respect to opioids, purport to be filed on behalf of a class of investors who acquired Walmart\n",
      "stock from March 30, 2016, through December 22, 2020. On May 11, 2021, the U.S. District Court in Delaware consolidated the class\n",
      "actions and appointed a lead plaintiff and lead counsel. The defendants moved to dismiss the consolidated securities class action on\n",
      "October 8, 2021. On October 14, 2022, plaintiffs filed an amended complaint, which revised the applicable putative class of investors\n",
      "to those who acquired Walmart stock from March 31, 2017, through December 22, 2020. On November 16, 2022, the Company moved\n",
      "to dismiss the amended complaint. That motion remains pending.\n",
      "Derivative Lawsuits: Abt v. Alvarez et al., USDC, Dist. of DE, 2/9/21; Nguyen v. McMillon et al., USDC, Dist. of DE, 4/16/21:\n",
      "Ontario Provincial Council of Carpenters' Pension Trust Fund et al. v. Walton et al., DE Court of Chancery, 9/27/21.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "model_embedding = OpenAIEmbeddings(\n",
    "    deployment = 'deployment-embedding-ada-002',\n",
    "    # model = ,\n",
    "    chunk_size = 16,\n",
    "    openai_api_version = '2023-08-01-preview',\n",
    "    openai_api_base = 'https://instance-text-risk-assessment.openai.azure.com/',\n",
    "    openai_api_key = os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    # openai_api_key = \"d7078cc13f3e46138215166baa8bced4\",\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "db_vector = FAISS.from_documents(\n",
    "    docs_chunks,\n",
    "    embedding = model_embedding,\n",
    "    \n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "retriever = db_vector.as_retriever()\n",
    "\n",
    "query = 'what is total profit?'\n",
    "# output = db_vector.similarity_search(query)\n",
    "# print(output[0].page_content)\n",
    "\n",
    "# db_vector.save_local(\"index_faiss_001\")\n",
    "db_vector_new = FAISS.load_local(\"index_faiss_001\", model_embedding)\n",
    "query = 'any legal issue?' \n",
    "# output = db_vector_new.similarity_search(query)\n",
    "print(output[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tool # 2 - RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.vectorstore.tool import VectorStoreQAWithSourcesTool\n",
    "\n",
    "tool_RAG = VectorStoreQAWithSourcesTool(\n",
    "    name = \"query_tool_RAG\",\n",
    "    description = \"\",\n",
    "    vectorstore = db_vector_new,\n",
    "    llm = model_llm,\n",
    "    verbose = True,\n",
    ")\n",
    "\n",
    "query = \"what is major legal issue?\"\n",
    "# output = tool_RAG(query)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x000001E76A1C7A90>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Tool_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_set = [\n",
    "    Tool.from_function(\n",
    "        name = \"Search from knowledge-base\",\n",
    "        func = tool_RAG.run,\n",
    "        description = \"answer questions based on knowledge-base (i.e. collected documents ) - RAG\"\n",
    "    ),\n",
    "    Tool.from_function(\n",
    "        name = \"Bing Search\",\n",
    "        func = tool_BingSearch.run,\n",
    "        description = \"answer questions based on web-search\"\n",
    "    ),  \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Agent - without memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent_without_memory = initialize_agent(\n",
    "    tools = tool_set,\n",
    "    llm = model_llm,\n",
    "    agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True,\n",
    "    handle_parsing_errors = True,\n",
    ")\n",
    "\n",
    "query = \"what is walmart's law suit 8 years ago?\"\n",
    "# agent_without_memory.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Agent - with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import AgentExecutor, ZeroShotAgent\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prefix = \"\"\"You are a helpful assistant.\n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"Begin to generate result. You need to work as a helpful financial assistant. \n",
    "{chat_history}\n",
    "Question: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ZeroShotAgent.create_prompt(\n",
    "    tools = tool_set,\n",
    "    prefix = prefix,\n",
    "    suffix = suffix,\n",
    "    input_variables = [\"input\", \"chat_history\", \"agent_scratchpad\"],\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key = \"chat_history\",\n",
    "    \n",
    "    )\n",
    "\n",
    "chain_llm = LLMChain(\n",
    "    llm = model_llm,\n",
    "    prompt = prompt,\n",
    ")\n",
    "\n",
    "agent = ZeroShotAgent(\n",
    "    llm_chain = chain_llm,\n",
    "    tools = tool_set,\n",
    "    verbose = True,\n",
    ")\n",
    "\n",
    "agent_with_memory = AgentExecutor.from_agent_and_tools(\n",
    "    agent = agent,\n",
    "    tools = tool_set,\n",
    "    verbose = True,\n",
    "    memory = memory,\n",
    "    handle_parsing_errors = True,\n",
    ")\n",
    "\n",
    "query = \"what is walmarts' law suit 5 years ago?\"\n",
    "# agent_with_memory.run(input = query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.title('Risk-Assessment Query Tool')\n",
    "user_web_input = st.text_input('What is the law suit of walmart?')\n",
    "if st.button('Generate Query Result') and user_web_input:\n",
    "    output = agent_without_memory.run(user_web_input)\n",
    "    st.wrtie(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tool - CSV Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: To find out the number of rows in the dataframe, I can use the `shape` attribute of the dataframe which returns a tuple with the number of rows and columns.\n",
      "\n",
      "Action: python_repl_ast\n",
      "Action Input: df.shape\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m(891, 12)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "\n",
      "Final Answer: There are 891 rows in the dataframe.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are 891 rows in the dataframe.'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install langchain_experimental\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from langchain_experimental.agents.agent_toolkits import create_csv_agent\n",
    "# from langchain.agents.agent_toolkits import create_csv_agent\n",
    "# from langchain.agents import create_csv_agent\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "\n",
    "agent_csv = create_csv_agent(\n",
    "    model_llm,\n",
    "    './data/csv/titanic.csv',\n",
    "    verbose = True,\n",
    "    agent_type = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    ")\n",
    "\n",
    "query = 'how many row are there?'\n",
    "agent_csv.run(query)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build database: Store .csv files into database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf = pd.read_csv(\"d:/00_data/data_bank_account/trans.csv\")\\ndf.columns = df.columns.str.strip()\\ndf.to_sql(\\n    \\'table_trans\\',\\n    sqlite3.connect(\\'bank_account_db.db\\'),\\n    if_exists = \\'replace\\'\\n)\\n\\n\\ndf_clients = pd.read_csv(\"d:/00_data/data_bank_account/clients.csv\")\\ndf_clients.columns = df_clients.columns.str.strip()\\ndf_clients.to_sql(\\n    \\'table_clients\\',\\n    sqlite3.connect(\\'./data/bank_account/bank_account_db.db\\'),\\n    if_exists = \\'replace\\'\\n)\\n\\ndf_loans = pd.read_csv(\"d:/00_data/data_bank_account/loans.csv\")\\ndf_loans.columns = df_loans.columns.str.strip()\\ndf_loans.to_sql(\\n    \\'table_loans\\',\\n    sqlite3.connect(\\'./data/bank_account/bank_account_db.db\\'),\\n    if_exists = \\'replace\\'\\n)\\n\\n'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install sqlite3\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\"\"\"\n",
    "df = pd.read_csv(\"d:/00_data/data_bank_account/card.csv\")\n",
    "df.columns = df.columns.str.strip()\n",
    "df.to_sql(\n",
    "    'table_card',\n",
    "    sqlite3.connect('bank_account_db.db'),\n",
    "    if_exists = 'replace'\n",
    ")\n",
    "\n",
    "df = pd.read_csv(\"d:/00_data/data_bank_account/order.csv\")\n",
    "df.columns = df.columns.str.strip()\n",
    "df.to_sql(\n",
    "    'table_order',\n",
    "    sqlite3.connect('bank_account_db.db'),\n",
    "    if_exists = 'replace'\n",
    ")\n",
    "\n",
    "df = pd.read_csv(\"d:/00_data/data_bank_account/accounts.csv\")\n",
    "df.columns = df.columns.str.strip()\n",
    "df.to_sql(\n",
    "    'table_accounts',\n",
    "    sqlite3.connect('bank_account_db.db'),\n",
    "    if_exists = 'replace'\n",
    ")\n",
    "\n",
    "df = pd.read_csv(\"d:/00_data/data_bank_account/district.csv\")\n",
    "df.columns = df.columns.str.strip()\n",
    "df.to_sql(\n",
    "    'table_district',\n",
    "    sqlite3.connect('bank_account_db.db'),\n",
    "    if_exists = 'replace'\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "df = pd.read_csv(\"d:/00_data/data_bank_account/trans.csv\")\n",
    "df.columns = df.columns.str.strip()\n",
    "df.to_sql(\n",
    "    'table_trans',\n",
    "    sqlite3.connect('bank_account_db.db'),\n",
    "    if_exists = 'replace'\n",
    ")\n",
    "\n",
    "\n",
    "df_clients = pd.read_csv(\"d:/00_data/data_bank_account/clients.csv\")\n",
    "df_clients.columns = df_clients.columns.str.strip()\n",
    "df_clients.to_sql(\n",
    "    'table_clients',\n",
    "    sqlite3.connect('./data/bank_account/bank_account_db.db'),\n",
    "    if_exists = 'replace'\n",
    ")\n",
    "\n",
    "df_loans = pd.read_csv(\"d:/00_data/data_bank_account/loans.csv\")\n",
    "df_loans.columns = df_loans.columns.str.strip()\n",
    "df_loans.to_sql(\n",
    "    'table_loans',\n",
    "    sqlite3.connect('./data/bank_account/bank_account_db.db'),\n",
    "    if_exists = 'replace'\n",
    ")\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build database: Test database for the existance of dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "curr = sqlite3.connect('./data/bank_account/bank_account_db.db').cursor()\n",
    "\n",
    "curr.execute('''SELECT * from table_clients''')\n",
    "for record in curr.fetchall():\n",
    "    print(record)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tool - SQL query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: sql_db_list_tables\n",
      "Action Input: \u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mtable_accounts, table_clients, table_disp, table_district, table_loans, table_loans_2\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe table that seems to hold account information is \"table_accounts\". I will now check the schema of this table to ensure I query the correct columns.\n",
      "Action: sql_db_schema\n",
      "Action Input: table_accounts\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\n",
      "CREATE TABLE table_accounts (\n",
      "\t\"index\" INTEGER, \n",
      "\t\"Unnamed: 0\" INTEGER, \n",
      "\taccount_id INTEGER, \n",
      "\tdistrict_id INTEGER, \n",
      "\tfrequency TEXT, \n",
      "\tdate TEXT\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from table_accounts table:\n",
      "index\tUnnamed: 0\taccount_id\tdistrict_id\tfrequency\tdate\n",
      "0\t0\t576\t55\tsettle_monthly\t1993-01-01\n",
      "1\t1\t3818\t74\tsettle_monthly\t1993-01-01\n",
      "2\t2\t704\t55\tsettle_monthly\t1993-01-01\n",
      "*/\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the structure of the table_accounts table and can write a query to count the number of rows in it.\n",
      "Action: sql_db_query_checker\n",
      "Action Input: SELECT COUNT(*) FROM table_accounts;\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe provided SQLite query does not exhibit any of the common mistakes listed. It is a simple query that counts the number of rows in the `table_accounts` table. Therefore, the original query is correct as it stands:\n",
      "\n",
      "```sql\n",
      "SELECT COUNT(*) FROM table_accounts;\n",
      "```\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2023-08-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 1 second. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThe query to count the rows in the table_accounts table is correct. I can now execute it to get the result.\n",
      "Action: sql_db_query\n",
      "Action Input: SELECT COUNT(*) FROM table_accounts;\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[(4500,)]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: There are 4500 rows in the accounts table.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "There are 4500 rows in the accounts table.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain.agents import create_sql_agent\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.agents.agent_types import AgentType\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///./data/bank_account/bank_account_db.db\")\n",
    "tool_sql = SQLDatabaseToolkit(\n",
    "    db = db,\n",
    "    llm = model_llm,\n",
    ")\n",
    "\n",
    "agent = create_sql_agent(\n",
    "    llm = model_llm,\n",
    "    toolkit = tool_sql,\n",
    "    agent_type = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True,\n",
    ")\n",
    "\n",
    "query = (\"how many rows are there in the accounts table?\")\n",
    "output = agent.run(query)\n",
    "print(output)\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the profit referred to is likely the \"Consolidated net income\" for the fiscal years mentioned in the document. The consolidated net income figures presented are as follows:\\n\\n- For fiscal year 2023: $11,292 million\\n- For fiscal year 2022: $13,940 million\\n- For fiscal year 2021: $13,706 million\\n\\nThese figures represent the net income after accounting for all expenses, including operating expenses, interest, taxes, and other gains and losses.'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context: \n",
    "{context}\n",
    "\n",
    "Question:{question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(\"what is the profit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install watermark\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -a \"Sudarshan Koirala\" -vmp langchain,openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sqlalchemy\n",
    "\"\"\"\n",
    "import sqlalchemy\n",
    "df = pd.read_sql_table(\n",
    "    \"table_loans\", \n",
    "    sqlalchemy.create_engine(\"sqlite:///./data/bank_account/bank_account_db.db\"),\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "\n",
    "db_sql = SQLDatabase.from_uri(\"sqlite:///./data/bank_account/card.db\")\n",
    "\n",
    "chain_sql = create_sql_query_chain(model_llm, db_sql)\n",
    "\n",
    "prompt = {\"question\":\"how many rows are there?\"}\n",
    "output = chain.invoke(prompt)\n",
    "\n",
    "\"\"\"\n",
    "# print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
